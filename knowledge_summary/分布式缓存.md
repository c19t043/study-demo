# 分布式缓存

## 上亿流量的商品详情页系统的多级缓存架构

很多人以为，做个缓存，其实就是用一下redis，访问一下，就可以了，简单的缓存

做复杂的缓存，支撑电商复杂的场景下的高并发的缓存，遇到的问题，非常非常之多，绝对不是说简单的访问一下redsi就可以了

采用三级缓存：nginx本地缓存+redis分布式缓存+tomcat堆缓存（本地缓存ehcache）的多级缓存架构

时效性要求非常高的数据：库存

一般来说，显示的库存，都是时效性要求会相对高一些，因为随着商品的不断的交易，库存会不断的变化

当然，我们就希望当库存变化的时候，尽可能更快将库存显示到页面上去，而不是说等了很长时间，库存才反应到页面上去

时效性要求不高的数据：商品的基本信息（名称、颜色、版本、规格参数，等等）

时效性要求不高的数据，就还好，比如说你现在改变了商品的名称，稍微晚个几分钟反应到商品页面上，也还能接受

商品价格/库存等时效性要求高的数据，而且种类较少，采取相关的服务系统每次发生了变更的时候，直接采取数据库和redis缓存双写的方案，这样缓存的时效性最高

商品基本信息等时效性不高的数据，而且种类繁多，来自多种不同的系统，采取MQ异步通知的方式，写一个数据生产服务，监听MQ消息，然后异步拉取服务的数据，更新tomcat jvm缓存+redis缓存

nginx+lua脚本做页面动态生成的工作，每次请求过来，优先从nginx本地缓存中提取各种数据，结合页面模板，生成需要的页面

如果nginx本地缓存过期了，那么就从nginx到redis中去拉取数据，更新到nginx本地

如果redis中也被LRU算法清理掉了，那么就从nginx走http接口到后端的服务中拉取数据，数据生产服务中，现在本地tomcat里的jvm堆缓存中找，ehcache，如果也被LRU清理掉了，
那么就重新发送请求到源头的服务中去拉取数据，然后再次更新tomcat堆内存缓存+redis缓存，并返回数据给nginx，nginx缓存到本地

## 多级缓存架构中每一层的意义，三级缓存架构（nginx本地缓存+redis分布式缓存+依赖服务本地缓存）

nginx本地缓存：缓存热点数据（数据缓存10分钟）
redis分布式缓存：缓存离散数据
依赖服务本地缓存：预防redis分布式缓存崩溃，请求直连数据库

nginx本地缓存，抗的是热数据的高并发访问，一般来说，商品的购买总是有热点的，比如每天购买iphone、nike、海尔等知名品牌的东西的人，总是比较多的

这些热数据，利用nginx本地缓存，由于经常被访问，所以可以被锁定在nginx的本地缓存内

大量的热数据的访问，就是经常会访问的那些数据，就会被保留在nginx本地缓存内，那么对这些热数据的大量访问，就直接走nginx就可以了

那么大量的访问，直接就可以走到nginx就行了，不需要走后续的各种网络开销了

redis分布式大规模缓存，抗的是很高的离散访问，支撑海量的数据，高并发的访问，高可用的服务

redis缓存最大量的数据，最完整的数据和缓存，1T+数据; 支撑高并发的访问，QPS最高到几十万; 可用性，非常好，提供非常稳定的服务

nginx本地内存有限，也就能cache住部分热数据，除了各种iphone、nike等热数据，其他相对不那么热的数据，可能流量会经常走到redis那里

利用redis cluster的多master写入，横向扩容，1T+以上海量数据支持，几十万的读写QPS，99.99%高可用性，那么就可以抗住大量的离散访问请求

tomcat jvm堆内存缓存，主要是抗redis大规模灾难的，如果redis出现了大规模的宕机，导致nginx大量流量直接涌入数据生产服务，那么最后的tomcat堆内存缓存至少可以再抗一下，不至于让数据库直接裸奔

同时tomcat jvm堆内存缓存，也可以抗住redis没有cache住的最后那少量的部分缓存

## Cache Aside Pattern

（1）读的时候，先读缓存，缓存没有的话，那么就读数据库，然后取出数据后放入缓存，同时返回响应

（2）更新的时候，先删除缓存，然后再更新数据库

## 为什么是删除缓存，而不是更新缓存呢？

举个例子，一个缓存涉及的表的字段，在1分钟内就修改了20次，那么缓存更新20次， 但是这个缓存在1分钟内就被读取了1次，有大量的冷数据

根据28黄金法则，20%的数据，占用了80%的访问量

实际上，如果你只是删除缓存的话，那么1分钟内，这个缓存不过就重新计算一次而已，开销大幅度降低
》》》》
每次数据过来，就只是删除缓存，然后修改数据库，如果这个缓存，在1分钟内只是被访问了1次，那么只有那1次，缓存是要被重新计算的，用缓存才去算缓存

其实删除缓存，而不是更新缓存，就是一个lazy计算的思想，不要每次都重新做复杂的计算，不管它会不会用到，而是让它到需要被使用的时候再重新计算

mybatis，hibernate，懒加载，思想

查询一个部门，部门带了一个员工的list，没有必要说每次查询部门，都里面的1000个员工的数据也同时查出来啊

80%的情况，查这个部门，就只是要访问这个部门的信息就可以了

先查部门，同时要访问里面的员工，那么这个时候只有在你要访问里面的员工的时候，才会去数据库里面查询1000个员工

## 数据库与缓存双写不一致问题

1. 第一：当要做数据修改时，先修改数据库中的数据，再删除缓存中的数据，如果修改数据库中数据成功，但删除缓存数据时失败，这时出现数据库与缓存数据不一致问题
2. 针对第一问题的解决思路，先删除缓存中的数据，再修改数据库中的数据，当删除缓存中的数据成功，既然数据库执行修改时报错，缓存数据为空，再重新从数据库中获取就可以了，不会出现数据库与缓存数据不一致问题
3. 第二：再执行先删缓存，再修改数据库中数据时，在缓存数据删除后，修改数据库数据前，有另外的读请求，发现缓存中的数据为空，从数据库中获取了未修改前的数据，重新放到了缓存中，这时即使数据库中数据修改成功，也与缓存中的数据不一致
4. 针对第二个问题，可以用这样的方案解决，将同一资源的读写操作串行化，在写数据时，不能对这个资源进行其他读写操作，读数据时，不能写，但可以并发读

## 数据库与缓存更新与读取操作进行异步串行化（数据强一致性解决方案）

在内存中建多个内存队列，一个队列对应一个线程，每个工作线程串行拿到对应的操作，然后一条条执行
这样的话，每次有请求过来，
如果是写请求，根据数据的唯一标识，将操作路由到对应的内存队列中，直接返回操作成功
如果是读请求，根据数据的唯一标识，将操作路由到对应的内存队列中，然后轮询等待缓存更新
在依次执行内存队列操作时：
如果是数据变更操作时，先执行删除缓存，然后执行更新数据库操作
如果是读数据操作时，先读缓存中的数据，如果缓存中有数据，则直接返回，如果缓存中的数据为空，则从数据库中获取
如果轮询等待缓存更新超过一定时长，则返回读取超时

## 高并发的场景下，该解决方案要注意的问题

（1）读请求长时阻塞
因为读写请求都会被发送到内存队列中，读写操作一个一个的执行，如果读操作前面，有多个写请求，那么执行读操作可能要等较长的时间
假如，内存队列中，读操作前面有10个写请求，每个写操作执行时间为20ms，那么执行读操作需要等待200ms，
当然具体的等待时长需要根据实际业务系统做压力测试，来调整这个数值
（2）读请求并发量过高
因为请求长时间阻塞，当读请求并发量过高的时候，服务中阻塞线请求过多
假如，一个读请求的阻塞时长是200ms，在这200ms内，有1000个请求，那么会阻塞1000个读请求
实际业务中，可以部署多个服务实例，分摊写请求，从而减少读请求的阻塞时长，进而解决当读请求并发过高，服务阻塞过多读请求的问题
（3）多服务实例部署的请求路由
对同一资源的请求，可以通过nginx，或ribbon，路由到同一服务上
（4）热点商品的路由问题，导致请求的倾斜
如果对某一资源读写请求特别高，如果秒杀场景，这些读写请求被路由到同一服务的同一个队列中，造成服务压力多大
对于这个问题，数据库与缓存的强一致性方案，很难解决，可以使用最终一致性解决方案，
不清除缓存中的数据，内存队列只发送写操作，读操作直接读内存中的数据
执行写操作时，直接修改换粗中的数据，将数据库的修改信息发送MQ，让下游的服务去执行数据库的数据修改操作

## 大型缓存全量更新问题

（1）网络耗费的资源大
（2）每次对redis都存取大数据，对redis的压力也比较大，redis的性能和吞吐量跟数据本身的大小有很大的关系，如果数据越大，那么可能导致redis的吞吐量就会急剧下降

## 缓存维度化解决方案

将网络中的大数据资源，按照不同维度拆分成多个小数据资源，然后发送多个异步请求来获取
比如商品详情页，可以拆分成商品介绍，商品分类，商品库存，商品规格等等

## 三层缓存架构，在架构中部署有多台nginx服务，如果提高nginx本地缓存命中率

1. nginx本地缓存命中率低的原因是对同一资源的多个请求可能分发到了不同的nginx服务上，多个数据的缓存分散在不同的nginx服务器上，导致能获取到缓存的概率低，然后将请求下发到redis分布式缓存中获取，提高了redis服务的压力
2. 解决方案：将三层架构中的nginx本地缓存，当做nginx应用层服务器，在nginx应用层服务器前部署nginx分发层服务器，用来对同一资源的请求分发到同一个nginx应用层服务器，提高nginx本地缓存命中率

## 分布式的缓存重建的并发问题

第一次访问的时候，其实在nginx本地缓存中是取不到的，所以会发送http请求到后端的缓存服务里去获取，会从redis中获取

拿到数据以后，会放到nginx本地缓存里面去，过期时间是10分钟

然后将所有数据渲染到模板中，返回模板

以后再来访问的时候，就会直接从nginx本地缓存区获取数据了

缓存数据生产 -> 有数据变更 -> 主动更新两级缓存（ehcache+redis）-> 缓存维度化拆分

分发层nginx + 应用层nginx -> 自定义流量分发策略提高缓存命中率

nginx shared dict缓存 -> 缓存服务 -> redis -> ehcache -> 渲染html模板 -> 返回页面

还差最后一个很关键的要点，就是如果你的数据在nginx -> redis -> ehcache三级缓存都不在了，可能就是被LRU清理掉了

这个时候缓存服务会重新拉去数据，去更新到ehcache和redis中

我们还遇到一个问题，就是说，如果缓存服务在本地的ehcache中都读取不到数据，那就恩坑爹了

这个时候就意味着，需要重新到源头的服务中去拉去数据，拉取到数据之后，赶紧先给nginx的请求返回，同时将数据写入ehcache和redis中

分布式重建缓存的并发冲突问题

重建缓存：比如我们这里，数据在所有的缓存中都不存在了（LRU算法弄掉了），就需要重新查询数据写入缓存，重建缓存

分布式的重建缓存，在不同的机器上，不同的服务实例中，去做上面的事情，就会出现多个机器分布式重建去读取相同的数据，然后写入缓存中

分布式重建缓存的并发冲突问题。。。。。。

## 分布式的缓存重建的并发问题（1）

1、流量均匀分布到所有缓存服务实例上

应用层nginx，是将请求流量均匀地打到各个缓存服务实例中的，可能咱们的eshop-cache那个服务，可能会部署多实例在不同的机器上

2、应用层nginx的hash，固定商品id，走固定的缓存服务实例

分发层的nginx的lua脚本，是怎么写的，怎么玩儿的，搞一堆应用层nginx的地址列表，对每个商品id做一个hash，然后对应用nginx数量取模

将每个商品的请求固定分发到同一个应用层nginx上面去

在应用层nginx里，发现自己本地lua shared dict缓存中没有数据的时候，就采取一样的方式，对product id取模，然后将请求固定分发到同一个缓存服务实例中去

这样的话，就不会出现说多个缓存服务实例分布式的去更新那个缓存了

留个作业，大家去做吧，这个东西，之前已经讲解果了，lua脚本几乎都是一模一样的，我们就不去做了，节省点时间

3、源信息服务发送的变更消息，需要按照商品id去分区，固定的商品变更走固定的kafka分区，也就是固定的一个缓存服务实例获取到

缓存服务，是监听kafka topic的，一个缓存服务实例，作为一个kafka consumer，就消费topic中的一个partition

所以你有多个缓存服务实例的话，每个缓存服务实例就消费一个kafka partition

所以这里，一般来说，你的源头信息服务，在发送消息到kafka topic的时候，都需要按照product id去分区

也就时说，同一个product id变更的消息一定是到同一个kafka partition中去的，也就是说同一个product id的变更消息，一定是同一个缓存服务实例消费到的

我们也不去做了，其实很简单，kafka producer api，里面send message的时候，多加一个参数就可以了，product id传递进去，就可以了

4、问题是，自己写的简易的hash分发，与kafka的分区，可能并不一致！！！

我们自己写的简易的hash分发策略，是按照crc32去取hash值，然后再取模的

关键你又不知道你的kafka producer的hash策略是什么，很可能说跟我们的策略是不一样的

拿就可能导致说，数据变更的消息所到的缓存服务实例，跟我们的应用层nginx分发到的那个缓存服务实例也许就不在一台机器上了

这样的话，在高并发，极端的情况下，可能就会出现冲突

5、分布式的缓存重建并发冲突问题发生了。。。

### 基于zookeeper分布式锁的解决方案

分布式锁，如果你有多个机器在访问同一个共享资源，那么这个时候，如果你需要加个锁，让多个分布式的机器在访问共享资源的时候串行起来

那么这个时候，那个锁，多个不同机器上的服务共享的锁，就是分布式锁

分布式锁当然有很多种不同的实现方案，redis分布式锁，zookeeper分布式锁

zk，做分布式协调这一块，还是很流行的，大数据应用里面，hadoop，storm，都是基于zk去做分布式协调

zk分布式锁的解决并发冲突的方案

（1）变更缓存重建以及空缓存请求重建，更新redis之前，都需要先获取对应商品id的分布式锁
（2）拿到分布式锁之后，需要根据时间版本去比较一下，如果自己的版本新于redis中的版本，那么就更新，否则就不更新
（3）如果拿不到分布式锁，那么就等待，不断轮询等待，直到自己获取到分布式的锁

比如：有3个产品服务，都监听了同一kafka集群中的1个topic，三个产品服务属于同一个消费组

## 缓存穿透，缓存雪崩,缓存击穿

+ 缓存穿透：访问一个不存在的key,缓存不起作用，请求发送到数据库层，当并发访问量大时，大量请求发送到数据库层，造层数据库压力过大，数据库直接挂掉
> 解决方案
> 采用布隆过滤器，使用一个足够大的bitmap，用于存储可能访问的key，不存在的key直接被过滤；
> 访问key未在DB查询到值，也将空值写进缓存，但可以设置较短过期时间。
+ 缓存击穿：一个存在的key，在缓存过期的那一刻，有大量并发请求，这些请求发送到数据库层，造成数据库压力过大，数据库直接挂掉
> 解决方案
> 在访问key之前，采用SETNX（set if not exists）来设置另一个短期key来锁住当前key的访问，访问结束再删除该短期key。
+ 缓存雪崩：同一时刻大量缓存失效，在并发访问量大时，大量请求发送到数据库层，造成数据库压力过大，数据库直接挂掉
> 解决方案
> 可以给缓存设置过期时间时加上一个随机值时间，使得每个key的过期时间分布开来，不会集中在同一时刻失效。